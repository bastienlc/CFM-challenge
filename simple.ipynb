{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    StackingRegressor,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from src.load import load_data\n",
    "from src.utils import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inspired by Franck Zibi's model :\n",
    "Level 0 : Extract features from the series of data\n",
    "Level 1 : Base classification with GradientBoostingClassifier\n",
    "Level 2 : Residual estimation of the probabilities with ensemble of models\n",
    "    - For each category\n",
    "        - GradientBoostingRegressor\n",
    "        - RandomForestRegressor\n",
    "        - KNeighborsRegressor\n",
    "        - MLPRegressor\n",
    "        - SVR\n",
    "Level 3 : Stacking of the residual estimations\n",
    "    - For each category\n",
    "        - StackingRegressor with GradientBoostingRegressor\n",
    "Level 4 : Nothing for now, but calibration on the test set could be done\n",
    "\"\"\"\n",
    "\n",
    "# Level 1\n",
    "base_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Level 2\n",
    "regressors = []\n",
    "for _ in range(24):\n",
    "    regressors.append(\n",
    "        [\n",
    "            (\"GradientBoostingRegressor\", GradientBoostingRegressor()),\n",
    "            (\"RandomForestRegressor\", RandomForestRegressor()),\n",
    "            (\"KNeighborsRegressor\", KNeighborsRegressor()),\n",
    "            (\"MLPRegressor\", MLPRegressor()),\n",
    "            (\"SVR\", SVR()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Level 3\n",
    "stacking_regressors = []\n",
    "for k in range(24):\n",
    "    stacking_regressors.append(\n",
    "        StackingRegressor(\n",
    "            estimators=regressors[k],\n",
    "            final_estimator=GradientBoostingRegressor(),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(X):\n",
    "    # TODO: Implement feature extraction from Sofiane's work\n",
    "    return X[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_test = load_data()\n",
    "X, y = X[:1000], y[:1000]\n",
    "\n",
    "X = compute_features(X)\n",
    "X_test = compute_features(X_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_y_val = np.eye(np.max(y_val) + 1)[y_val]\n",
    "one_hot_y_train = np.eye(np.max(y_train) + 1)[y_train]\n",
    "\n",
    "print(\"Training base classifier...\")\n",
    "base_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_train_residuals = one_hot_y_train - base_classifier.predict_proba(X_train)\n",
    "\n",
    "for k in range(24):\n",
    "    print(f\"Training regressor {k}...\")\n",
    "    stacking_regressors[k].fit(X_train, y_train_residuals[:, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"runs/simple/base_classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(base_classifier, f)\n",
    "\n",
    "for k in range(24):\n",
    "    with open(f\"runs/simple/stacking_regressor_{k}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(stacking_regressors[k], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"runs/simple/base_classifier.pkl\", \"rb\") as f:\n",
    "    base_classifier = pickle.load(f)\n",
    "\n",
    "for k in range(24):\n",
    "    with open(f\"runs/simple/stacking_regressor_{k}.pkl\", \"rb\") as f:\n",
    "        stacking_regressors[k] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_base_pred = base_classifier.predict_proba(X_val)\n",
    "y_val_residuals = np.zeros_like(y_val_base_pred)\n",
    "for k in range(24):\n",
    "    y_val_residuals[:, k] = stacking_regressors[k].predict(X_val)\n",
    "y_val_pred = y_val_base_pred + y_val_residuals\n",
    "y_val_pred = np.argmax(y_val_pred, axis=1)\n",
    "\n",
    "print(\"Validation accuracy:\", (y_val_pred == y_val).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_base_pred = base_classifier.predict_proba(X_test)\n",
    "y_test_residuals = np.zeros_like(y_test_base_pred)\n",
    "for k in range(24):\n",
    "    y_test_residuals[:, k] = stacking_regressors[k].predict(X_test)\n",
    "y_test_pred = y_test_base_pred + y_test_residuals\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "save(y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

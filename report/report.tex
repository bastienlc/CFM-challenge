\documentclass[switch, 11pt]{article}

\usepackage{preprint} 
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{resizegather}
\usepackage[numbers,square]{natbib}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage[colorlinks = true,
    linkcolor = purple,
    urlcolor  = blue,
    citecolor = cyan,
    anchorcolor = black]{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lineno}
\usepackage{float}
\usepackage{multicol}
\usepackage[shortlabels]{enumitem}
\usepackage{float}
\usepackage{subfloat}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{authblk}
\usepackage{graphics}
\usepackage[font=small,labelfont=bf]{caption}

\setlength{\belowcaptionskip}{-10pt}
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\DeclareMathOperator{\leakyrelu}{LeakyReLU}

\bibliographystyle{unsrtnat}
\setlist[enumerate,1]{leftmargin=2em}
\titlespacing\section{0pt}{0.1ex plus 0.2ex minus 0.1ex}{0.1ex plus 0.2ex minus 0.1ex}
\titlespacing\subsection{0pt}{0.1ex plus 0.2ex minus 0.1ex}{0.1ex plus 0.2ex minus 0.1ex}

\renewcommand*{\Authfont}{\bfseries}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\minimize}{minimize}

\title{Stock Classification from High Frequency Market Data}
\author[1]{Sofiane Ezzehi}
\author[1]{Bastien Le Chenadec}
\affil[1]{Ã‰cole des Ponts ParisTech, Master MVA}

\begin{document}

\maketitle

\begin{contribstatement}
\end{contribstatement}

\begin{multicols}{2}
    \section{Introduction}

    The goal of this challenge is to predict the stock corresponding to a given order book. Each sample is a chronological sequence of 100 events of orders for a given stock. To make this task more challenging, a lot of the data is missing, and some properties have been normalized.

    The first part of this challenge was devoluted to correctly dealing with the temporal and categorical aspects of the data. Then we focused on dealing with the change of distribution of the data between the training and the test set.

    In this report, we describe the data, the different models we used, the training procedure as well as the results we obtained.

    \section{Data overview}

    Each sample in the dataset is constituted of 100 events of orders for a given stock. There are 24 different stocks which are equally distributed in the training, validation, and test sets. There are 160800 samples in the training set and 80600 samples in the test set.

    % table describing the data
    \begin{table}[H]
        \begin{center}
            \resizebox{\columnwidth}{!}{
                \begin{tabular}{|c|c|p{5cm}|}
                    \hline
                    \textbf{Feature} & \textbf{Type} & \textbf{Description}                                                    \\
                    \hline
                    Venue            & Categorical   & The venue where the order was placed.                                   \\
                    \hline
                    Order id         & Integer       & A unique identifier, which can be used to retrace updates to the order. \\
                    \hline
                    Action           & Categorical   & The type of action (new, delete, update).                               \\
                    \hline
                    Side             & Categorical   & The side of the order (buy, sell).                                      \\
                    \hline
                    Price            & Float         & The price of the order.                                                 \\
                    \hline
                    Bid              & Float         & The best buying price for the stock.                                    \\
                    \hline
                    Ask              & Float         & The best selling price for the stock.                                   \\
                    \hline
                    Bid size         & Float         & The number of shares available at the best buying price.                \\
                    \hline
                    Ask size         & Float         & The number of shares available at the best selling price.               \\
                    \hline
                    Trade            & Categorical   & Whether a trade occured or not.                                         \\
                    \hline
                    Flux             & Integer       & The quantity of shares for this order.                                  \\
                    \hline
                \end{tabular}
            }
        \end{center}
        \caption{Data description.}
        \label{tab:data}
    \end{table}

    Each event is described by 11 features, as described in Table \ref{tab:data}. There are 4 categorical features, 5 continuous features, one integer feature (flux) and the last integer feature (order id) also has some categorical properties as it links the different events of the same order.

    \subsection{Visualization}
    To get a better understanding of the data, we performed an in-depth series of visualizations. The idea was to see if some features -original or derived- clearly separated the different stocks. To do so, we made use of different types of statistical plots (boxplots, histograms...) where we hoped, at each step, to see if one or several stocks were clearly identifiable.

    To test the relevancy of the features that we derived, we performed a random forest classification on the training set. We then can look at the feature importance to see which features are the most discriminative.

    Let's first take a look at some of the interesting features we derived. We need to keep in mind that the goal is to extract an ensemble of features that would be discriminative for different stocks. Also, the following list is not exhaustive, and we will only present some decently clear results.

    \paragraph{Bid-ask spread}
    The very first measure we naturally looked at was the bid-ask spread. Since it is a measure of the liquidity of the stock, we expected it to be a particularly good indicator of the stock. We plotted, on figure \ref{fig:spread}, the boxplot of the bid-ask spread for each stock. More precisely, for each stock, we considered all the bid-ask spreads of all the observations of the stock in the training set. Furthermore, we only collected the points where a limit order was placed, updated, deleted or traded. While we can see that the spread does not seem to completely separate the stocks, it is still a good indicator of the stock with a decent variety of boxplot shapes. As we will see in the results section, the bid-ask spread was actually the most important feature in the random forest classification.

    \paragraph{Price outliers (number and price value)}
    An \textit{a priori} good feature we derived was the characteristics of the price outliers. We can see on figure \ref{fig:nb_outliers} representing the percentage of price outliers per number of ticks, that the stocks have relatively different distributions. We plotted on figure \ref{fig:boxplot_outliers} the boxplot of the percentage of price outliers for each stock, over all available observations in the training set. We can see that the stocks do not really separate well. Nevertheless, we get more interesting results when we look at the actual values of the price outliers. We plotted on figures \ref{fig:outliers_bid} and \ref{fig:outliers_ask} the boxplot of the bid and ask price outliers for each stock. For example, we can see that stocks 7 and 20, as well as stocks 5, 8, 15, 17, 19 and 23 distinguish themselves from the others by having a higher percentage of outliers. This type of result is what we are looking for, since it gives additional discriminative information about the stocks.

    \paragraph{Volume}
    A very straightforward feature, which is actually given in the data, is the volume of the orders. We plotted on figure \ref{fig:volume} the boxplot of Bid and Ask sizes for each stock. We can see that, as in the case of the bid-ask spread, the stocks separate decently well.

    \paragraph{Price outliers (flux)}
    Similarly, we looked at the flux of the orders that were price outliers. We distinguished $2\times 2$ types of outliers: Bid/Ask and Addition/Deletion. We plotted on figures \ref{fig:flux_bid} and \ref{fig:flux_ask} the corresponding boxplots. Here, we can see that a variety of stocks are clearly distinguishable. A striking example is stock 20, which stands out alone in the bid addition outliers. Therefore, if the test set has a similar distribution, we can expect our model to perform almost perfectly on this stock.

    \paragraph{Other unconclusive features} Among the other features we looked at, we can mention the trade proportion per stock, the trade price and volume per stock, the bid-ask spread distinguished by venue or the proportion of limit orders. None of these features were particularly discriminative.

    \subsection{Preprocessing}

    \subsection{Graph construction}

    To deal with the temporal and categorical aspects of the data, one idea is to represent the data as a graph that better represents the relationships between the different events. After a bit of trial and error, we made the following arbitrary choices to construct a graph representing a sample :
    \begin{enumerate}
        \item The graph is undirected.
        \item Each event is a node in the graph.
        \item Each venue corresponds to a connex component in the graph.
        \item If two events happen successively at a venue, there is an edge between them.
        \item If two events have the same order id, there is an edge between them.
    \end{enumerate}

    The separation of the graph into connex components corresponding to the venues is motivated by the high frequency nature of the data. Indeed, very few actors can react with high speed to the events happening in another venue, so it makes sense to assume that the events happening in different venues are somewhat independent. Note that the order id is unique to a venue so there should be no edge between the different venues.

    The features that are not encoded in the structure of the graph can be placed on the nodes and the edges.
    \begin{itemize}
        \item Node features: price, bid, ask, bid size, ask size, flux.
        \item Edge features: action, side, trade.
    \end{itemize}
    We also add the venue and a time feature to the nodes, otherwise the model would not be able to distinguish between the different venues, and would not know in which direction the time flows.

    \section{Method}

    In this section, we describe the different models we used. We experimented with three main types of models: recurrent neural networks which are well suited for sequential data, graph neural networks which exploit the graph representation of the data, and statistical models that exploit features extracted from the sequence of events. From the start we knew that our final prediction would be an ensemble of models, so we explored different types of models to maximize the diversity of the ensemble. Indeed the diversity should improve the robustness of the predictions, especially given the change of distribution between the training and the test set.

    \subsection{Graph Attention Networks}

    Graph Attention Networks (GAT) \cite{velickovic-2018} have been shown to be effective in many tasks. Like other graph neural networks, GATs aggregate information from the neighbors of each node to compute its embedding. The main difference with other models is that GATs use an attention mechanism to weight the neighbors of each node. Specifically we used the improved version of GATs suggested in \cite{brody-2021}.

    Let $G$ be an undirected graph with $N$ nodes denoted $\llbracket1, N\rrbracket$. Let $d_1$ be the dimension of the node embeddings, and $h_1,\dots,h_N\in\R^{d_1}$ be the said embeddings. Let $d_2$ be the dimension of the edge embeddings, and $\left\{e_{i,j} \; | 1\leq i,j\leq N\right\}$ be the said embeddings. Let $W_1\in\R^{d'\times d_1}$, $W_2\in\R^{d'\times d_2}$ and $a\in\R^{d'}$. The attention weights are :
    \begin{equation}
        w(h_i,h_j, e_{i,j}) = a^T \leakyrelu(W_1h_i + W_1h_j + W_2e_{i,j})
    \end{equation}
    The attention weights are normalized using the softmax operator :
    \begin{equation}
        \alpha_{ij} = \frac{\exp(w(h_i,h_j, e_{i,j}))}{\sum_{k\in\mathcal{N}_i}\exp(w(h_i,h_k, e_{i,k}))}
    \end{equation}
    where $\mathcal{N}_i$ denotes the set of neighbors of node $i$ in $G$. The embedding of node $i$ is then computed as :
    \begin{equation}
        h_i' = \leakyrelu\left(\sum_{j\in\mathcal{N}_i}\alpha_{ij}W_1h_j\right)
    \end{equation}
    In general we will use multi-head attention, with $K$ heads, $a^{(1)},\dots,a^{(K)}\in\R^{d'/K}$, $W_1^{(1)},\dots,W_1^{(K)}\in\R^{d'/K\times d_1}$ and $W_2^{(1)},\dots,W_2^{(K)}\in\R^{d'/K\times d_2}$ :
    \begin{equation}
        h_i' = \leakyrelu\left(\frac{1}{K}\sum_{k=1}^K\sum_{j\in\mathcal{N}_i}\alpha_{ij}^{(k)}W_1^{(k)}h_j\right)
    \end{equation}
    Furthermore, we will stack multiple GAT layers to obtain a deeper model. We may also apply a multi-layer perceptron to the embeddings of the last layer to obtain a more expressive representation. This model is easily parallelizable which is useful for mini-batch training.

    \subsection{Other graph models}

    We started out with GAT models because we already had some experience with them, and they were very effective. However, we also trained other graph models to diversify our ensemble (as long as they supported edge features in the graph). Here is an exhaustive list of models that gave us acceptable results and were added to the ensemble (in no particular order) :
    \begin{itemize}
        \item Generalized GNN \cite{li-2020}
        \item Pathfinder Discovery Network \cite{rozemberczki-2021}
        \item Principal Neighbourhood Aggregation \cite{corso-2020}
        \item Graph transformer \cite{shi-2020}
        \item General GNN \cite{you-2020}
    \end{itemize}
    \section{Results}
    \subsection{Statistical models}
    \paragraph{Random Forest Classifier}
    Throughout our experiments, we found the random forest classifier handy for feature selection, as well as for training a model purely based on hand-crafted features.

    We present in this section the results of $2$ random forest classifiers that we trained. The first model was trained on the Bid-Ask spread, the Bid and Ask volume, the number of price outliers, the price outliers, the flux of the price outliers, and the proportion of each venues on which the orders were placed. The second model was only trained on a subset of the features of the first model, which were the Bid-Ask spread, the Bid and Ask volume and the venue proportion.

    We obtained a very important result that framed our approach for the rest of the challenge. We found that the first model had a $49\%$ accuracy on the validation set, while the second model had a $28\%$ accuracy. However, both models performed very similarly on the test set, with an accuracy of respectively $22\%$ and $20\%$. This result is very important because it gives us a clear indication that the "outlier" features, while being very discriminative on the training set, are almost irrelevant on the test set. At this point, we had $2$ choices: either we could eliminate the outlier features from our ensemble, or we could try to find a model that would be able to detect how the "outlier" features distribution changed in the test set.
    \newpage
    \bibliography{bibliography}

\end{multicols}

\newpage
\appendix

\vspace{5mm}
\begin{center}
    {\Large \bfseries Appendix} \\
\end{center}


\end{document}